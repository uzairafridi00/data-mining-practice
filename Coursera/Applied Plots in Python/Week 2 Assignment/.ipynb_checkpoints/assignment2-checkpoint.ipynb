{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e45d05a7e6705e7bc18fff5d5890ef58",
     "grade": false,
     "grade_id": "cell-44ca835c70f3040a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false; // disable scroll bar when displaying Folium map\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae6111747a75bdb5c5393f44d1045577",
     "grade": false,
     "grade_id": "cell-c676d66924c74eea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "Before working on this assignment please read these instructions fully. In the submission area, you will notice that you can click the link to **Preview the Grading** for each step of the assignment. This is the criteria that will be used for peer grading. Please familiarize yourself with the criteria before beginning the assignment.\n",
    "\n",
    "The data for this assignment comes from a subset of The National Centers for Environmental Information (NCEI) [Global Historical Climatology Network daily (GHCNd)](https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily) (GHCN-Daily). The GHCN-Daily is comprised of daily climate records from thousands of land surface stations across the globe - it's a wonderfully large dataset to play with! In particular, you will be asked to use data from the Ann Arbor Michigan location (my home!). and this is stored in the file: `assets/fb441e62df2d58994928907a91895ec62c2c42e6cd075c2700843b89.csv`\n",
    "\n",
    "Each row in this datafile corresponds to a single observation from a weather station, and has the following variables:\n",
    "* **id** : station identification code\n",
    "* **date** : date in YYYY-MM-DD format (e.g. 2012-01-24 = January 24, 2012)\n",
    "* **element** : indicator of element type\n",
    "    * TMAX : Maximum temperature (tenths of degrees C)\n",
    "    * TMIN : Minimum temperature (tenths of degrees C)\n",
    "* **value** : data value for element (tenths of degrees C)\n",
    "\n",
    "For this assignment, you must:\n",
    "\n",
    "1. Read the documentation and familiarize yourself with the dataset, then write a python notebook which plots line graphs of the record high and record low temperatures by day of the year over the period 2005-2014. The area between the record high and record low temperatures for each day should be shaded.\n",
    "2. Overlay a scatter of the 2015 data for any points (highs and lows) for which the ten year record (2005-2014) record high or record low was broken in 2015. (Based on the graph, do you think extreme weather is getting more frequent in 2015?)\n",
    "3. Watch out for leap days (i.e. February 29th), it is reasonable to remove these points from the dataset for the purpose of this visualization.\n",
    "4. Make the visual nice! Leverage principles from the first module in this course when developing your solution. Consider issues such as legends, labels, and chart junk.\n",
    "\n",
    "I've written some steps I think would be good to go through, but there are other ways to solve this assignment so feel free to explore the pandas library! What I really want to see is an image that looks like this sketch I drew at my desk:\n",
    "\n",
    "![](assets/chris_sketch.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8d9355fc55599cd2ad34fdd140baac1",
     "grade": false,
     "grade_id": "cell-f01cb0e8645e7c07",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-1.12.4.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2 {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 500.0px;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2 = L.map(\n",
       "                &quot;map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2&quot;,\n",
       "                {\n",
       "                    center: [41.9164, -84.0158],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 9,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_b8a171e37d7a412fcc1b19d8d172f9b0 = L.tileLayer(\n",
       "                &quot;https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;Data by \\u0026copy; \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://openstreetmap.org\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e, under \\u003ca target=\\&quot;_blank\\&quot; href=\\&quot;http://www.openstreetmap.org/copyright\\&quot;\\u003eODbL\\u003c/a\\u003e.&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 18, &quot;maxZoom&quot;: 18, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_be5d1a8f74dcfab8b645cf8831a14a62 = L.marker(\n",
       "                [41.9164, -84.0158],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_76541287249d6f96e25df47c858bcec6 = L.marker(\n",
       "                [42.2875, -83.7611],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_7f704b806a3bbfbad5ab4b7750871f00 = L.marker(\n",
       "                [42.2417, -83.6933],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_d38bd0cc6d7af4bfc7d2b5a7c1f51806 = L.marker(\n",
       "                [42.2947, -83.7108],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_ab16d4e38a68fa0b0b04adb24d01a7d2 = L.marker(\n",
       "                [41.84, -83.8608],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_f7f0d6fb8b47ca08e434e7f0fc5a6371 = L.marker(\n",
       "                [42.0636, -83.4358],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_ef7e2b10624867ba8e51642a1f647c84 = L.marker(\n",
       "                [42.3264, -84.0133],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_fc7e0ba24cfbe6e3f84d9769fef023a2 = L.marker(\n",
       "                [41.9553, -83.6489],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_e26195e61f8adb158b31184271cd1f2f = L.marker(\n",
       "                [42.4344, -83.9858],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_25ae5804f7501d29d94e19bf9bb7cb08 = L.marker(\n",
       "                [42.1508, -84.0236],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_d6f226971cc68e7a73414feb647628b3 = L.marker(\n",
       "                [42.0664, -83.6186],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_a999ef25b7c7010ec5e101f63f41342a = L.marker(\n",
       "                [42.0811, -83.6769],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_caa9093d56af4c15cb0c97c6cecc0d70 = L.marker(\n",
       "                [41.9069, -83.4158],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_81ad5bd68d3c4d4c6a6e9fe9fab10a77 = L.marker(\n",
       "                [41.9497, -83.28],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_ce6ed3af3590f529d1bc50fc82ecd13c = L.marker(\n",
       "                [42.1611, -83.7819],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_367808ec88988ad433e827772b5438a8 = L.marker(\n",
       "                [42.1236, -83.82],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_26b36c5b4b547324a70fb8b500c993af = L.marker(\n",
       "                [41.8069, -83.5831],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_469277ae3a3173d95e13b40346961617 = L.marker(\n",
       "                [42.0028, -83.9336],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_73a795f361f67c071e0e2e9ec95c1d35 = L.marker(\n",
       "                [42.0283, -84.1108],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_ab9e3841f6401f8ed3116a95c6cded9b = L.marker(\n",
       "                [42.4356, -83.7831],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_1e960d8fbb25b187956c8f7a0339f155 = L.marker(\n",
       "                [41.5631, -83.4764],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_c60180db17bc7808a87322dbdc56d47f = L.marker(\n",
       "                [42.2667, -84.4667],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_ed45a9b30bbe33a400c1b0cbeda3b400 = L.marker(\n",
       "                [42.2333, -83.5333],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "    \n",
       "            var marker_94d8aa4dd64f26d14139fd48c80ed23b = L.marker(\n",
       "                [42.2228, -83.7444],\n",
       "                {}\n",
       "            ).addTo(map_6c44e2e2e2eb9dd1bd0f75c0b9ad6be2);\n",
       "        \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x1facb2403d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  I'll be using the folium package to render the data into a map in Jupyter.\n",
    "\n",
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "# get the location information for this dataset\n",
    "df = pd.read_csv('assets/BinSize_d400.csv')\n",
    "station_locations_by_hash = df[df['hash'] == 'fb441e62df2d58994928907a91895ec62c2c42e6cd075c2700843b89']\n",
    "\n",
    "# get longitude and lattitude to plot\n",
    "lons = station_locations_by_hash['LONGITUDE'].tolist()\n",
    "lats = station_locations_by_hash['LATITUDE'].tolist()\n",
    "\n",
    "# plot on a beautiful folium map\n",
    "my_map = folium.Map(location = [lats[0], lons[0]], height = 500,  zoom_start = 9)\n",
    "for lat, lon in zip(lats, lons):\n",
    "    folium.Marker([lat, lon]).add_to(my_map)\n",
    "\n",
    "# render map in Jupyter\n",
    "display(my_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2ee9d3c5ac53844b3ed48aa157f6204",
     "grade": false,
     "grade_id": "cell-695e4689bc5509b6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 1\n",
    "Load the dataset and transform the data into Celsius (refer to documentation) then extract all of the rows which have minimum or maximum temperatures.\n",
    "\n",
    "__hint: when I did this step I had two DataFrame objects, each with ~80,000 entries in it__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbfe393a2232a653ebbd81e518237a83",
     "grade": false,
     "grade_id": "cell-f508059dd84e9b7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Element</th>\n",
       "      <th>Data_Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USW00094889</td>\n",
       "      <td>2014-11-12</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00208972</td>\n",
       "      <td>2009-04-29</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USC00200032</td>\n",
       "      <td>2008-05-26</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USC00205563</td>\n",
       "      <td>2005-11-11</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USC00200230</td>\n",
       "      <td>2014-02-27</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>-106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID        Date Element  Data_Value\n",
       "0  USW00094889  2014-11-12    TMAX          22\n",
       "1  USC00208972  2009-04-29    TMIN          56\n",
       "2  USC00200032  2008-05-26    TMAX         278\n",
       "3  USC00205563  2005-11-11    TMAX         139\n",
       "4  USC00200230  2014-02-27    TMAX        -106"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('assets/fb441e62df2d58994928907a91895ec62c2c42e6cd075c2700843b89.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Element</th>\n",
       "      <th>Data_Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USW00094889</td>\n",
       "      <td>2014-11-12</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>-5.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USC00208972</td>\n",
       "      <td>2009-04-29</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>13.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USC00200032</td>\n",
       "      <td>2008-05-26</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>136.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USC00205563</td>\n",
       "      <td>2005-11-11</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>59.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USC00200230</td>\n",
       "      <td>2014-02-27</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>-76.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID        Date Element  Data_Value\n",
       "0  USW00094889  2014-11-12    TMAX       -5.56\n",
       "1  USC00208972  2009-04-29    TMIN       13.33\n",
       "2  USC00200032  2008-05-26    TMAX      136.67\n",
       "3  USC00205563  2005-11-11    TMAX       59.44\n",
       "4  USC00200230  2014-02-27    TMAX      -76.67"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this code cell, transform the Data_Value column\n",
    "def f(x):\n",
    "    x = (x - 32) * 5/9\n",
    "    return float(\"{:.2f}\".format(x))\n",
    "\n",
    "df['Data_Value'] = df['Data_Value'].apply(f)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "76f2478088402765c38ed2b9db771916",
     "grade": false,
     "grade_id": "cell-c5718635688cb408",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 2\n",
    "In order to visualize the data we would plot the min and max data for each day of the year between the years 2005 and 2014 across all weather stations. But we also need to find out when the min or max temperature in 2015 falls below the min or rises above the max for the previous decade.\n",
    "\n",
    "If you did step 1 you have two Series objects with min and max times for the years 2005 through 2015. You can use Pandas `groupby` to create max and min temperature Series objects across all weather stations for each day of these years, and you can deal with the records for February 29 (the leap year) by dropping them.\n",
    "\n",
    "__hint: when I finished this step, I had two DataFrame objects, each with exactly 4015 observations in them__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3629\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16024\\2473538813.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Convert Date column to datetime type\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Filter the data to only include records from 2005 to 2015\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;34m'2005-01-01'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;34m'2016-01-01'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3505\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3506\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3631\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3632\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3633\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Date'"
     ]
    }
   ],
   "source": [
    "# Convert Date column to datetime type\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Filter the data to only include records from 2005 to 2015\n",
    "df = df[(df['Date'] >= '2005-01-01') & (df['Date'] < '2016-01-01')]\n",
    "\n",
    "# Drop February 29th\n",
    "df = df[~((df['Date'].dt.month == 2) & (df['Date'].dt.day == 29))]\n",
    "\n",
    "# Pivot the table to have the columns as dates and rows as weather stations\n",
    "df_pivoted = df.pivot(index='ID', columns='Date', values='Data_Value')\n",
    "\n",
    "# Create two separate DataFrames for 2005-2014 and 2015\n",
    "df_decade = df_pivoted.loc[:, '2005-01-01':'2014-12-31']\n",
    "df_2015 = df_pivoted.loc[:, '2015-01-01':]\n",
    "\n",
    "# Group by month and day and find the min and max for each day across all years\n",
    "grouped_decade = df_decade.groupby([df_decade.index, df_decade.columns.month, df_decade.columns.day]).agg({'min', 'max'})\n",
    "\n",
    "# Flatten the multi-level column index\n",
    "grouped_decade.columns = ['_'.join(map(str, col)).strip() for col in grouped_decade.columns.values]\n",
    "\n",
    "# Reset the index\n",
    "grouped_decade = grouped_decade.reset_index()\n",
    "\n",
    "# Create separate columns for month and day\n",
    "grouped_decade['Month'] = grouped_decade['Date_month_day'].apply(lambda x: x.month)\n",
    "grouped_decade['Day'] = grouped_decade['Date_month_day'].apply(lambda x: x.day)\n",
    "\n",
    "# Drop the month-day column\n",
    "grouped_decade = grouped_decade.drop('Date_month_day', axis=1)\n",
    "\n",
    "# Group by month and day and find the min and max for each day in 2015\n",
    "grouped_2015 = df_2015.groupby([df_2015.index, df_2015.columns.month, df_2015.columns.day]).agg({'min', 'max'})\n",
    "\n",
    "# Flatten the multi-level column index\n",
    "grouped_2015.columns = ['_'.join(map(str, col)).strip() for col in grouped_2015.columns.values]\n",
    "\n",
    "# Reset the index\n",
    "grouped_2015 = grouped_2015.reset_index()\n",
    "\n",
    "# Create separate columns for month and day\n",
    "grouped_2015['Month'] = grouped_2015['Date_month_day'].apply(lambda x: x.month)\n",
    "grouped_2015['Day'] = grouped_2015['Date_month_day'].apply(lambda x: x.day)\n",
    "\n",
    "# Drop the month-day column\n",
    "grouped_2015 = grouped_2015.drop('Date_month_day', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0edac9c92f1b79eb9b21f302a3259c5e",
     "grade": false,
     "grade_id": "cell-d3a1a2647a47fe31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 3\n",
    "Now that you have grouped the daily max and min temperatures for each day of the years 2005 through 2015, you can separate out the data for 2015. Then you can use the Pandas `groupby` function to find the max and min of the temperature data for each __day of the year__ for the 2005-2014 data.\n",
    "\n",
    "__hint: at the end of this step I had two DataFrames, one of maximum and the other of minimum values, which each had 365 observations in them. I also had another pair of similar DataFrames but only for the year 2015.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_2015 = tmax.loc['2015']\n",
    "tmin_2015 = tmin.loc['2015']\n",
    "tmax_grouped = tmax[tmax.index.year != 2015].groupby([tmax.index.month, tmax.index.day]).max()\n",
    "tmin_grouped = tmin[tmin.index.year != 2015].groupby([tmin.index.month, tmin.index.day]).min()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7de066a05b833f7ded9353ee2215ba8",
     "grade": false,
     "grade_id": "cell-25711f5fdbe49515",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 4\n",
    "Now it's time to plot! You need to explore matplotlib in order to plot line graphs of the min and max temperatures for the years 2005 through 2014 and to scatter plot __only__ the daily 2015 temperatures that exceeded those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from calendar import month_abbr\n",
    "\n",
    "# put your plotting code here!\n",
    "\n",
    "# create figure and axis objects with subplots()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot the max and min temperature for each day of the year from 2005-2014\n",
    "ax.plot(grouped_data['tmax'].max(), label='Max Temp')\n",
    "ax.plot(grouped_data['tmin'].min(), label='Min Temp')\n",
    "\n",
    "# add x-axis and y-axis labels\n",
    "ax.set_xlabel('Day of the Year')\n",
    "ax.set_ylabel('Temperature (°C)')\n",
    "\n",
    "# add a title\n",
    "ax.set_title('Temperature Summary (2005-2014)')\n",
    "\n",
    "# add a legend\n",
    "ax.legend()\n",
    "\n",
    "# show plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# create figure and axis objects with subplots()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plot the max and min temperature for each day of the year from 2005-2014\n",
    "ax.plot(grouped_data['tmax'].max(), label='Max Temp')\n",
    "ax.plot(grouped_data['tmin'].min(), label='Min Temp')\n",
    "\n",
    "# scatter plot the 2015 temperature data that exceeded the 2005-2014 temperature range\n",
    "ax.scatter(tmax_2015[tmax_2015 > grouped_data['tmax'].max()], marker='o', color='red', label='High in 2015')\n",
    "ax.scatter(tmin_2015[tmin_2015 < grouped_data['tmin'].min()], marker='o', color='blue', label='Low in 2015')\n",
    "\n",
    "# add x-axis and y-axis labels\n",
    "ax.set_xlabel('Day of the Year')\n",
    "ax.set_ylabel('Temperature (°C)')\n",
    "\n",
    "# add a title\n",
    "ax.set_title('Temperature Summary (2005-2014) with 2015 Outliers')\n",
    "\n",
    "# add a legend\n",
    "ax.legend()\n",
    "\n",
    "# show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mooc_adswpy_v1_assignment2"
   ]
  },
  "etc_active_cell": 6,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
